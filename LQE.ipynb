{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LQE.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "cXbu6hucBV-Y",
        "J3Nk5yGeCere",
        "ZggjMQRyKOas",
        "jUz68aLdKnYe",
        "YnGd6HrE7yni",
        "MBL9jq5IbCQv",
        "USOiX53As8hH",
        "Z3OsJGf1aZKV"
      ],
      "authorship_tag": "ABX9TyP9yG4G9+x+L8T0VTZy3+SE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/freshmandarina/Link-quality-estimation/blob/main/LQE.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**You can skip getting traces if you have the initial_dataset from my github page**"
      ],
      "metadata": {
        "id": "Sh2_E5WG0zY-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Getting traces"
      ],
      "metadata": {
        "id": "cXbu6hucBV-Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#1\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mGJWmeEYlwo3",
        "outputId": "e541f043-1a15-4d87-81fa-497780e3fa40"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WQ9CimZfmFSx",
        "outputId": "6037d120-d722-487c-93ba-0a2f6b0e14b1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "drive  sample_data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#2\n",
        "%cd /content/drive/MyDrive/IJS/project_folder/link-quality-estimation/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SQV9cLX2mKQo",
        "outputId": "3ec6c0e1-c0f2-4d93-d061-b6f40d0eb5d9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/IJS/project_folder/link-quality-estimation\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#ze imam\n",
        "#! git clone https://github.com/sensorlab/link-quality-estimation.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CfBpm50CoT4B",
        "outputId": "c7315aae-6eba-49a6-dc4d-5c8f4357b121"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'link-quality-estimation'...\n",
            "remote: Enumerating objects: 5327, done.\u001b[K\n",
            "remote: Counting objects: 100% (29/29), done.\u001b[K\n",
            "remote: Compressing objects: 100% (21/21), done.\u001b[K\n",
            "remote: Total 5327 (delta 10), reused 20 (delta 6), pack-reused 5298\u001b[K\n",
            "Receiving objects: 100% (5327/5327), 197.50 MiB | 10.76 MiB/s, done.\n",
            "Resolving deltas: 100% (162/162), done.\n",
            "Checking out files: 100% (4156/4156), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#3\n",
        "!pip install -e ."
      ],
      "metadata": {
        "id": "V9m56o8ypfPY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#4\n",
        "!python setup.py install"
      ],
      "metadata": {
        "id": "pgc4dDfWqT34"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!find / -type d -name \"dataset-1-rutgers_wifi\"\n",
        "#!rm -r /usr/local/lib/python3.7/dist-packages/LQE-0.1-py3.7.egg/output/datasets/dataset-1-rutgers_wifi\n",
        "#!python ./datasets/trace1_Rutgers/transform.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oQpCrTdGrbbA",
        "outputId": "2eabdfe8-4453-42a4-e99b-8014e68911db"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "find: ‘/proc/27/task/27/net’: Invalid argument\n",
            "find: ‘/proc/27/net’: Invalid argument\n",
            "find: ‘/proc/227/task/227/net’: Invalid argument\n",
            "find: ‘/proc/227/net’: Invalid argument\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets.trace1_Rutgers.transform import get_traces\n",
        "#traces = list(get_traces())"
      ],
      "metadata": {
        "id": "fvpjldjs3xqg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "myEuuWnvy6TF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#My definitions"
      ],
      "metadata": {
        "id": "VNtJSbYisyK8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## basic"
      ],
      "metadata": {
        "id": "hC2UgG2CjgJT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import List, Tuple, Generator, Iterator\n",
        "from enum import Enum\n",
        "from os import path\n",
        "\n",
        "import os\n",
        "#os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"\n",
        "\n",
        "import numpy as np\n",
        "import scipy as sp\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Set custom matplotlib themes\n",
        "#try:\n",
        "#    plt.style.use(['science', 'ieee'])\n",
        "#except OSError:\n",
        "#    pass\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler, RobustScaler, QuantileTransformer, MinMaxScaler\n",
        "from sklearn.ensemble import IsolationForest, BaggingClassifier\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_predict\n",
        "from sklearn import metrics\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import OneClassSVM, SVC\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.neighbors import LocalOutlierFactor\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV, ParameterGrid\n",
        "\n",
        "from sklearn import metrics, preprocessing, ensemble, linear_model, model_selection, neighbors, pipeline, svm, base\n",
        "\n",
        "\n",
        "#from datasets.trace1_Rutgers.transform import get_traces, dtypes, get_traces2\n",
        "#from datasets.trace1_Rutgers import NOISE_SOURCES\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers, backend as K, callbacks\n",
        "\n",
        "#my addons\n",
        "from random import seed, randint\n",
        "from itertools import compress\n",
        "\n",
        "tf.config.set_soft_device_placement(True)\n",
        "\n",
        "import joblib\n",
        "from joblib import Parallel, delayed\n",
        "\n",
        "from pandas import DataFrame"
      ],
      "metadata": {
        "id": "YCvhE42_jgJT"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def give_start_stop_length():\n",
        "    rng = np.random.default_rng()\n",
        "    seed(rng.choice(1000))\n",
        "\n",
        "    START = randint(0, 200)\n",
        "    LENGTH = randint(50, 200)\n",
        "    if 300 < START + LENGTH:\n",
        "      over = START + LENGTH - 300\n",
        "      START = START - over\n",
        "    STOP = START + LENGTH\n",
        "\n",
        "    if STOP >= 300:\n",
        "      START, STOP, LENGTH = give_start_stop_length()\n",
        "\n",
        "    return START, STOP, LENGTH"
      ],
      "metadata": {
        "id": "ypddEWrE5aqQ"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "SEED = 0xDEADBEEF\n",
        "\n",
        "NUM_OF_SAMP = 2123\n",
        "TX_POWER = 10.0 # dBm\n",
        "RSSI_BASE = -95.0 # dBm; not sure for this one. Most of the Atheros' have -95\n",
        "FREQ = 2450.0 # MHz\n"
      ],
      "metadata": {
        "id": "hbscZrqzjgJT"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def name2coords(name: str) -> Tuple[float, float]:\n",
        "    \"\"\"Name of the node contains information about it's relative position in testbed\"\"\"\n",
        "    part = name[len('node'):] # extract only numeric part 'node3-2' -> '3-2'\n",
        "    coords = [float(i) for i in part.split('-')]\n",
        "    return tuple(coords)\n"
      ],
      "metadata": {
        "id": "xX5YRezUjgJT"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_unique_pairs(df = None, i = 0):\n",
        "    #get cut\n",
        "    if i == 0:\n",
        "      df = get_cut_dataset()\n",
        "\n",
        "    pairs = df.apply(lambda row: (row['src'], row['dst'], row['noise']), axis=1).unique()\n",
        "    return pairs"
      ],
      "metadata": {
        "id": "1su5AxjQjgJU"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_initial_dataset() -> List[pd.DataFrame]:\n",
        "    \n",
        "    return initial_dataset\n",
        "    #old code\n",
        "    def process(df: pd.DataFrame):\n",
        "        assert len(df.index) == 300\n",
        "\n",
        "        if df.received.sum() != 300:\n",
        "            return None\n",
        "\n",
        "        df['src'] = df['src'].apply(name2coords) # str -> coordinates\n",
        "        df['dst'] = df['dst'].apply(name2coords) # str -> coordinates\n",
        "        df['seq'] = df.index # preserve sequence number\n",
        "        df['rss'] = df.rssi + RSSI_BASE # received signal strength\n",
        "\n",
        "        # Fix #1: Initial values are abnormal on every link. Replace them to avoid misclassification\n",
        "        mean, stdev = np.mean(df[df.seq > 3].rss), np.std(df[df.seq > 3].rss)\n",
        "        df.loc[df.seq < 3, 'rss'] = np.random.normal(mean, stdev, size=3)\n",
        "\n",
        "        return df\n",
        "\n",
        "    # Parallel link transformation\n",
        "    traces = get_traces2(n_jobs=-1)\n",
        "    dfs = Parallel(n_jobs=-1)(delayed(process)(df) for df in traces)\n",
        "\n",
        "    # Merge them at the end\n",
        "    output = pd.concat(dfs, ignore_index=True)\n",
        "    return output"
      ],
      "metadata": {
        "id": "RFsS7LJHjgJU"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_cut_dataset() -> List[pd.DataFrame]:\n",
        "    df = get_initial_dataset() \n",
        "\n",
        "    #geting the parameters for cutting dataframe\n",
        "    x_start = START\n",
        "    x_stop = STOP\n",
        "\n",
        "    #geting the df to start from START index and end on END index. We cut the rest\n",
        "    new_df = []\n",
        "    new_df = pd.DataFrame(new_df)\n",
        "    for _ in range(NUM_OF_SAMP):\n",
        "      temp = new_df\n",
        "      new_df = df[x_start:x_stop]\n",
        "      x_start = x_start + 300\n",
        "      x_stop = x_stop + 300\n",
        "      new_df = temp.append(new_df,ignore_index=True)\n",
        "    new_df = repair_seq_cut(new_df)\n",
        "\n",
        "    return new_df"
      ],
      "metadata": {
        "id": "tT7UL9dozRzT"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def repair_seq_cut(df):\n",
        "  \n",
        "    #user to repair seq numbers for cut dataset\n",
        "    df = df.drop(labels=['seq'], axis=1)\n",
        "    seq = pd.DataFrame([])\n",
        "    for n in range(2123):\n",
        "      temp = seq\n",
        "      seq = pd.DataFrame({ 'seq' : range(1, LENGTH+1)}).astype(int)\n",
        "      seq = temp.append(seq,ignore_index=True)\n",
        "    df['seq'] = seq\n",
        "    return df"
      ],
      "metadata": {
        "id": "KdoB_EBsOLa_"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def give_randomizing_array(i=0):\n",
        "\n",
        "    #used to get the START STOP and LENGTH for each sample in our dataset\n",
        "    array = np.empty((NUM_OF_SAMP, 3))\n",
        "\n",
        "    x_start = START\n",
        "    x_stop = STOP\n",
        "    x_length = LENGTH\n",
        "\n",
        "    if i == 1:\n",
        "      for n in range(NUM_OF_SAMP):\n",
        "        x_start, x_stop, x_length = give_start_stop_length()\n",
        "        array[n][0] = x_start\n",
        "        array[n][1] = x_stop\n",
        "        array[n][2] = x_length\n",
        "    if i == 0:\n",
        "      for n in range(NUM_OF_SAMP):\n",
        "        array[n][0] = x_start\n",
        "        array[n][1] = x_stop\n",
        "        array[n][2] = x_length\n",
        "  \n",
        "    return array\n",
        "    "
      ],
      "metadata": {
        "id": "f9PkYEOOvE81"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_random_dataset(i=0):\n",
        "\n",
        "    #used to cut every dataset sample into a random length\n",
        "    rng = np.random.default_rng()\n",
        "\n",
        "    df = get_initial_dataset()\n",
        "\n",
        "    lengths = pd.DataFrame([])\n",
        "    new_df = pd.DataFrame([])\n",
        "    array = give_randomizing_array(i).astype(int)\n",
        "\n",
        "    for n in range(NUM_OF_SAMP):\n",
        "      \n",
        "      x_start = array[n][0] + 300*n\n",
        "      x_stop = array[n][1] + 300*n\n",
        "      x = [array[n][2]]*array[n][2]\n",
        "      lengths = lengths.append(x,ignore_index=True)\n",
        "\n",
        "      temp = new_df\n",
        "      new_df = df[x_start:x_stop]\n",
        "      new_df = temp.append(new_df,ignore_index=True)\n",
        "\n",
        "    new_df['length'] = lengths\n",
        "    new_df = repair_seq(new_df,array)\n",
        "    return new_df, array"
      ],
      "metadata": {
        "id": "UA3cBhPrZjDJ"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def repair_seq(df,array):\n",
        "\n",
        "    #used to repair seq numbers for cut dataset\n",
        "    df = df.drop(labels=['seq'], axis=1)\n",
        "    seq = pd.DataFrame([])\n",
        "    for n in range(2123):\n",
        "      temp = seq\n",
        "      seq = pd.DataFrame({ 'seq' : range(1, array[n][2]+1)}).astype(int)\n",
        "      seq = temp.append(seq,ignore_index=True)\n",
        "    df['seq'] = seq\n",
        "    return df"
      ],
      "metadata": {
        "id": "DGigJooDK5oq"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def check_line(affected_links):\n",
        "    #this is so we can get rid of duplicates\n",
        "    check_line = []\n",
        "    check = 0\n",
        "    for n in range(len(affected_links)):\n",
        "      for m in range(len(check_line)):\n",
        "        if check_line[m] == affected_links[n]:\n",
        "          check = 1\n",
        "      if check == 0:\n",
        "        check_line.append(affected_links[n])\n",
        "      check = 0\n",
        "\n",
        "    return check_line"
      ],
      "metadata": {
        "id": "uO9m1CFR8jyu"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## cut/uncut - spikes, norecovery, recovery, slow"
      ],
      "metadata": {
        "id": "YnGd6HrE7yni"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def norecovery_anomaly_injector_cut(scaler=None, random_state=None) -> pd.DataFrame:\n",
        "    # Get ready-to-use prepared dataset for anomaly injection\n",
        "    df = get_cut_dataset()\n",
        "    # Random number generator\n",
        "    rng = np.random.default_rng(random_state)\n",
        "\n",
        "    # Now, find unique pairs\n",
        "    unique_pairs = get_unique_pairs() #df.apply(lambda row: (row['src'], row['dst'], row['noise']), axis=1).unique()\n",
        "    n_pairs = len(unique_pairs)\n",
        "\n",
        "    df['anomaly'] = False\n",
        "    df['type']= 1\n",
        "    df['original'] = df.rss\n",
        "\n",
        "    # approx. 33.3% of links will have anomaly\n",
        "    n_anomaly_pairs = int(n_pairs // 3)\n",
        "\n",
        "    # No-recovery anomaly has randomized start, which will appear randomly between 2/3 (originaly 200th) and 14/15 (o. 280th) of a sample\n",
        "    # Let's assume there is no more the 1/3 of malicious samples.\n",
        "    randomize_start = lambda: rng.integers(round((2*LENGTH)/3), round((14*LENGTH)/15), endpoint=True)\n",
        "    affected_links = rng.choice(unique_pairs, n_anomaly_pairs)\n",
        "    affected_links = check_line(affected_links)\n",
        "\n",
        "    for src, dst, noise in affected_links:\n",
        "        query = (df.src==src) & (df.dst==dst) & (df.noise==noise)\n",
        "        size = len(df[query].index)\n",
        "        assert size == LENGTH, f'something is wrong. Size {size} is not {LENGTH}'\n",
        "\n",
        "        start = randomize_start()\n",
        "        query = query & (df.seq >= start)\n",
        "        df.loc[query, 'anomaly'] = True\n",
        "\n",
        "        ## Now inject actual annomaly\n",
        "\n",
        "        # get delta between median and noise floor.\n",
        "        delta = abs(RSSI_BASE - df.loc[query, 'rss'].median())\n",
        "\n",
        "        # Drop should be around 30 or less, depending on delta\n",
        "        delta = delta if delta < 30 else (30 - rng.uniform(0, 3))\n",
        "\n",
        "        # Apply drop where anomaly should have appeared\n",
        "        df.loc[query & df.anomaly, 'rss'] -= delta\n",
        "\n",
        "        # Fix values below noise floor\n",
        "        df.loc[query & df.anomaly & (df['rss'] < RSSI_BASE), 'rss'] = RSSI_BASE\n",
        "\n",
        "    assert np.any(df.original != df.rss)\n",
        "\n",
        "    # Scale links afterwards\n",
        "    if scaler:\n",
        "        for src, dst, noise in unique_pairs:\n",
        "            query = (df.src==src) & (df.dst==dst) & (df.noise==noise)\n",
        "            df.loc[query, ['rss']] = scaler.fit_transform(df.loc[query, ['rss']])\n",
        "\n",
        "    assert len(df.index) != 0\n",
        "    return df"
      ],
      "metadata": {
        "id": "9_n5-5zS7yno"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def recovery_anomaly_injector_cut(scaler=None, random_state=None) -> pd.DataFrame:\n",
        "    # Get ready-to-use prepared dataset for anomaly injection\n",
        "    df = get_cut_dataset()\n",
        "\n",
        "    # Random number generator\n",
        "    rng = np.random.default_rng(random_state)\n",
        "\n",
        "    # Now, find unique pairs\n",
        "    unique_pairs = get_unique_pairs() #df.apply(lambda row: (row['src'], row['dst'], row['noise']), axis=1).unique()\n",
        "    n_pairs = len(unique_pairs)\n",
        "\n",
        "    df['anomaly'] = False\n",
        "    df['type']= 2\n",
        "    df['original'] = df.rss\n",
        "\n",
        "    # approx. 33.3% of links will have anomaly\n",
        "    n_anomaly_pairs = int(n_pairs // 3)\n",
        "\n",
        "    # Random drop starts between 1/12 (o. 25th) and 11/12(o. 275th) packet\n",
        "    randomize_start = lambda: rng.integers(round(LENGTH/12), round((11*LENGTH)/12), endpoint=True)\n",
        "\n",
        "    # Randomly recovers after 3th to 1/15 (o. 20th) packets\n",
        "    randomize_length = lambda: rng.integers(3, round(LENGTH/15), endpoint=True)\n",
        "\n",
        "    # Randomly select affected links\n",
        "    affected_links = rng.choice(unique_pairs, n_anomaly_pairs)\n",
        "    affected_links = check_line(affected_links)\n",
        "\n",
        "    for src, dst, noise in affected_links:\n",
        "        query = (df.src==src) & (df.dst==dst) & (df.noise==noise)\n",
        "        size = len(df[query].index)\n",
        "        assert size == LENGTH, f'something is wrong. Size {size} is not {LENGTH}'\n",
        "\n",
        "        start = randomize_start()\n",
        "        stop = start + randomize_length()\n",
        "        query = query & (df.seq >= start) & (df.seq <= stop)\n",
        "        df.loc[query, 'anomaly'] = True\n",
        "\n",
        "        ## Now actually inject anomalies\n",
        "\n",
        "        # get delta between median and noise floor.\n",
        "        delta = abs(RSSI_BASE - df.loc[query, 'rss'].median())\n",
        "\n",
        "        # Drop should be around 30 or less, depending on delta\n",
        "        delta = delta if delta < 30 else (30 - rng.uniform(0, 3))\n",
        "\n",
        "        # Apply drop where anomaly should have appeared\n",
        "        df.loc[query & df.anomaly, 'rss'] -= delta\n",
        "\n",
        "        # Fix values below noise floor\n",
        "        df.loc[query & df.anomaly & (df['rss'] < RSSI_BASE), 'rss'] = RSSI_BASE\n",
        "\n",
        "    assert np.any(df.original != df.rss)\n",
        "\n",
        "    # Scale links afterwards\n",
        "    if scaler:\n",
        "        for src, dst, noise in unique_pairs:\n",
        "            query = (df.src==src) & (df.dst==dst) & (df.noise==noise)\n",
        "            df.loc[query, ['rss']] = scaler.fit_transform(df.loc[query, ['rss']])\n",
        "\n",
        "    assert len(df.index) != 0\n",
        "    return df\n"
      ],
      "metadata": {
        "id": "B1BPPF_67yno"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pandas.core.indexers import length_of_indexer\n",
        "def spike_anomaly_injector_cut(scaler=None, random_state=None) -> pd.DataFrame:\n",
        "\n",
        "    # Get cut-ready-to-use prepared dataset for anomaly injection\n",
        "    df = get_cut_dataset()\n",
        "    # Random number generator\n",
        "    rng = np.random.default_rng(random_state)\n",
        "\n",
        "    # Now, find unique pairs\n",
        "    unique_pairs = get_unique_pairs() #df.apply(lambda row: (row['src'], row['dst'], row['noise']), axis=1).unique()\n",
        "    n_pairs = len(unique_pairs)\n",
        "\n",
        "    df['anomaly'] = False\n",
        "    df['type']= 3\n",
        "    df['original'] = df.rss\n",
        "\n",
        "    # approx. 33.3% of links will have anomaly\n",
        "    n_anomaly_pairs = int(n_pairs // 3)\n",
        "\n",
        "    # The probability for spike anomaly to appear is:\n",
        "    anomaly_probability = 3. / LENGTH\n",
        "\n",
        "    # Randomly select links, which will be altered\n",
        "    affected_links = rng.choice(unique_pairs, n_anomaly_pairs)\n",
        "    affected_links = check_line(affected_links)\n",
        "\n",
        "    for src, dst, noise in affected_links:\n",
        "        query = (df.src==src) & (df.dst==dst) & (df.noise==noise)\n",
        "        size = len(df[query].index)\n",
        "        assert size == LENGTH, f'something is wrong. Size {size} is not {LENGTH}'\n",
        "        df.loc[query, 'anomaly'] = np.random.choice([False, True], p=[1. - anomaly_probability, anomaly_probability], size=LENGTH)\n",
        "\n",
        "\n",
        "        ## Now inject actual annomaly\n",
        "\n",
        "        # get delta between median and noise floor.\n",
        "        delta = abs(RSSI_BASE - df.loc[query, 'rss'].median())\n",
        "\n",
        "        # Drop should be around 30 or less, depending on delta\n",
        "        delta = delta if delta < 30 else (30 - rng.uniform(0, 3))\n",
        "\n",
        "        # Apply drop where anomaly should have appeared\n",
        "        df.loc[query & df.anomaly, 'rss'] -= delta\n",
        "\n",
        "        # Fix values below noise floor\n",
        "        df.loc[query & df.anomaly & (df['rss'] < RSSI_BASE), 'rss'] = RSSI_BASE\n",
        "\n",
        "    assert np.any(df.original != df.rss)\n",
        "\n",
        "    # Scale links afterwards\n",
        "    if scaler:\n",
        "        for src, dst, noise in unique_pairs:\n",
        "            query = (df.src==src) & (df.dst==dst) & (df.noise==noise)\n",
        "            df.loc[query, ['rss']] = scaler.fit_transform(df.loc[query, ['rss']])\n",
        "\n",
        "    assert len(df.index) != 0\n",
        "    return df"
      ],
      "metadata": {
        "id": "d7UXGG9Csec0"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def slow_anomaly_injector_cut(scaler=None, random_state=None) -> pd.DataFrame:\n",
        "    # Get ready-to-use prepared dataset for anomaly injection\n",
        "    df = get_cut_dataset()\n",
        "\n",
        "    # Random number generator\n",
        "    rng = np.random.default_rng(random_state)\n",
        "\n",
        "    # Now, find unique pairs\n",
        "    unique_pairs = get_unique_pairs() #df.apply(lambda row: (row['src'], row['dst'], row['noise']), axis=1).unique()\n",
        "    n_pairs = len(unique_pairs)\n",
        "\n",
        "    df['anomaly'] = False\n",
        "    df['type']= 4\n",
        "    df['original'] = df.rss\n",
        "\n",
        "    # approx. 33.3% of links will have anomaly\n",
        "    n_anomaly_pairs = int(n_pairs // 3)\n",
        "\n",
        "    # randomize start of slow degradation between LENGTH/30 (o. 10th) and LENGTH/3 (o. 200th) packet\n",
        "    rand_start = lambda: rng.integers(0, round(LENGTH/15), endpoint=True)\n",
        "    rand_duration = lambda: rng.integers(round(LENGTH/2), round((14*LENGTH)/15), endpoint=True)\n",
        "    rand_rate = lambda: rng.uniform(0.5, 1.5, size=LENGTH) / -LENGTH\n",
        "\n",
        "    def slope(seq, rate, start):\n",
        "        # General curve\n",
        "        curve = rate * (seq - start)\n",
        "        curve[curve > 0] = 0  # Correction #1: Remove values above 0\n",
        "        #curve[curve < (rate * (end - start))] = rate * (end - start) # Correction #2: stop falling after \"end\" value\n",
        "\n",
        "        # Sanity checks\n",
        "        assert np.all(rate < 0), f'Rate should have been negative. Got {rate}'\n",
        "\n",
        "        return curve\n",
        "\n",
        "    # Randomly select affected links\n",
        "    affected_links = rng.choice(unique_pairs, n_anomaly_pairs)\n",
        "    affected_links = check_line(affected_links)\n",
        "\n",
        "    for src, dst, noise in affected_links:\n",
        "        query = (df.src==src) & (df.dst==dst) & (df.noise==noise)\n",
        "        size = len(df[query].index)\n",
        "        assert size == LENGTH, f'something is wrong. Size {size} is not {LENGTH}'\n",
        "\n",
        "        start = rand_start()\n",
        "        end = start + rand_duration()\n",
        "        rate = rand_rate()\n",
        "\n",
        "        df.loc[query & (df.seq >= start), 'anomaly'] = True\n",
        "        df.loc[query, 'rss'] += slope(seq=df.loc[query].seq, rate=rate, start=start)\n",
        "        #df.loc[query, 'rss'] += factor(seq=df.loc[query,:].seq, offset=start)\n",
        "\n",
        "    assert np.any(df.original != df.rss)\n",
        "\n",
        "    # Scale links afterwards\n",
        "    if scaler:\n",
        "        for src, dst, noise in unique_pairs:\n",
        "            query = (df.src==src) & (df.dst==dst) & (df.noise==noise)\n",
        "            df.loc[query, ['rss']] = scaler.fit_transform(df.loc[query, ['rss']])\n",
        "\n",
        "    assert len(df.index) != 0\n",
        "    return df\n"
      ],
      "metadata": {
        "id": "8BNsSOaz7ynp"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_all_cut():\n",
        "\n",
        "  df1 = norecovery_anomaly_injector_cut()\n",
        "  df2 = recovery_anomaly_injector_cut()\n",
        "  df3 = spike_anomaly_injector_cut()\n",
        "  df4 = slow_anomaly_injector_cut()\n",
        "\n",
        "  df = df1.append(df2).append(df3).append(df4,ignore_index=True)\n",
        "\n",
        "  np_rss = df['rss'].to_numpy()\n",
        "  np_anomaly = df['anomaly'].to_numpy()\n",
        "  np_type = df['type'].to_numpy()\n",
        "\n",
        "  #cut anomaly and type\n",
        "  anomaly_np = []\n",
        "  t = 0\n",
        "  for n in range(NUM_OF_SAMP*4):\n",
        "    for i in range(LENGTH):\n",
        "      t=0\n",
        "      if np_anomaly[LENGTH*n + i] == True:\n",
        "        anomaly_np.append(True)\n",
        "        t=1\n",
        "        break\n",
        "    if t == 0:\n",
        "      anomaly_np.append(False)\n",
        "\n",
        "  type_np = []\n",
        "  for n in range(len(anomaly_np)):\n",
        "    if anomaly_np[n] == True:\n",
        "      type_np.append(np_type[LENGTH*n])\n",
        "    else:\n",
        "      type_np.append(0)\n",
        "    \n",
        "  #get mask\n",
        "  mask_np = []\n",
        "  for n in range(len(np_anomaly)):\n",
        "    if np_anomaly[n] == True:\n",
        "      mask_np.append(1)\n",
        "    else:\n",
        "      mask_np.append(0)\n",
        "\n",
        "\n",
        "  #append everything\n",
        "  dataset_np = np.empty((NUM_OF_SAMP*4, LENGTH*2 +2))\n",
        "  for n in range(NUM_OF_SAMP*4):\n",
        "    np_rss_temp = np_rss[LENGTH*n:LENGTH*(n+1)]\n",
        "    np_rss_temp = np.append(np_rss_temp,anomaly_np[n])\n",
        "    np_rss_temp = np.append(np_rss_temp,type_np[n])\n",
        "    dataset_np[n] = np.append(np_rss_temp,mask_np[LENGTH*n:LENGTH*(n+1)])\n",
        "\n",
        "  dataset_df = pd.DataFrame(dataset_np)\n",
        "  dataset_df = pd.DataFrame.rename(dataset_df, columns={LENGTH : \"anomaly\"})\n",
        "  dataset_df = pd.DataFrame.rename(dataset_df, columns={(LENGTH+1) : \"type\"})\n",
        "\n",
        "  #del dataset_df['Unnamed: 0']\n",
        "\n",
        "  return dataset_df\n"
      ],
      "metadata": {
        "id": "CbWTdFDJJVax"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## random - spikes, norecovery, recovery, slow"
      ],
      "metadata": {
        "id": "MBL9jq5IbCQv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def norecovery_anomaly_injector(scaler=None, random_state=None) -> pd.DataFrame:\n",
        "    # Get ready-to-use prepared dataset for anomaly injection\n",
        "    df, lengths_original = get_random_dataset(1)\n",
        "\n",
        "    # Random number generator\n",
        "    rng = np.random.default_rng(random_state)\n",
        "\n",
        "    # Now, find unique pairs\n",
        "    unique_pairs = get_unique_pairs(df, 1) #df.apply(lambda row: (row['src'], row['dst'], row['noise']), axis=1).unique()\n",
        "    n_pairs = len(unique_pairs)\n",
        "    \n",
        "    \n",
        "    df['anomaly'] = False\n",
        "    df['type']= 1\n",
        "    df['original'] = df.rss\n",
        "\n",
        "    # approx. 33.3% of links will have anomaly\n",
        "    n_anomaly_pairs = int(n_pairs // 3)\n",
        "\n",
        "    # No-recovery anomaly has randomized start, which will appear randomly between 2/3 (originaly 200th) and 14/15 (o. 280th) of a sample\n",
        "    # Let's assume there is no more the 1/3 of malicious samples.\n",
        "    \n",
        "    affected_links = rng.choice(unique_pairs, n_anomaly_pairs)\n",
        "    affected_links = check_line(affected_links)\n",
        "\n",
        "    for src, dst, noise in affected_links:\n",
        "\n",
        "        query = (df.src==src) & (df.dst==dst) & (df.noise==noise)\n",
        "        size = len(df[query].index)\n",
        "        randomize_start = lambda: rng.integers(round((2*size)/3), round((14*size)/15), endpoint=True)\n",
        "        \n",
        "        # return df,query,size,temp_length\n",
        "        # assert size == temp_length, f'something is wrong. Size {size} is not {temp_length}'\n",
        "\n",
        "        start = randomize_start()\n",
        "        query = query & (df.seq >= start)\n",
        "        df.loc[query, 'anomaly'] = True\n",
        "\n",
        "        ## Now inject actual annomaly\n",
        "\n",
        "        # get delta between median and noise floor.\n",
        "        delta = abs(RSSI_BASE - df.loc[query, 'rss'].median())\n",
        "\n",
        "        # Drop should be around 30 or less, depending on delta\n",
        "        delta = delta if delta < 30 else (30 - rng.uniform(0, 3))\n",
        "\n",
        "        # Apply drop where anomaly should have appeared\n",
        "        df.loc[query & df.anomaly, 'rss'] -= delta\n",
        "\n",
        "        # Fix values below noise floor\n",
        "        df.loc[query & df.anomaly & (df['rss'] < RSSI_BASE), 'rss'] = RSSI_BASE\n",
        "\n",
        "    assert np.any(df.original != df.rss)\n",
        "\n",
        "    # Scale links afterwards\n",
        "    if scaler:\n",
        "        for src, dst, noise in unique_pairs:\n",
        "            query = (df.src==src) & (df.dst==dst) & (df.noise==noise)\n",
        "            df.loc[query, ['rss']] = scaler.fit_transform(df.loc[query, ['rss']])\n",
        "\n",
        "    assert len(df.index) != 0\n",
        "    return df, lengths_original"
      ],
      "metadata": {
        "id": "eR1A1-aBbCQx"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def recovery_anomaly_injector(scaler=None, random_state=None) -> pd.DataFrame:\n",
        "    # Get ready-to-use prepared dataset for anomaly injection\n",
        "    df, lengths_original = get_random_dataset(1)\n",
        "\n",
        "    # Random number generator\n",
        "    rng = np.random.default_rng(random_state)\n",
        "\n",
        "    # Now, find unique pairs\n",
        "    unique_pairs = get_unique_pairs(df, 1) #df.apply(lambda row: (row['src'], row['dst'], row['noise']), axis=1).unique()\n",
        "    n_pairs = len(unique_pairs)\n",
        "\n",
        "    df['anomaly'] = False\n",
        "    df['type']= 2\n",
        "    df['original'] = df.rss\n",
        "\n",
        "    # approx. 33.3% of links will have anomaly\n",
        "    n_anomaly_pairs = int(n_pairs // 3)\n",
        "\n",
        "    # Randomly select affected links\n",
        "    affected_links = rng.choice(unique_pairs, n_anomaly_pairs)\n",
        "    affected_links = check_line(affected_links)\n",
        "\n",
        "    for src, dst, noise in affected_links:\n",
        "        query = (df.src==src) & (df.dst==dst) & (df.noise==noise)\n",
        "        size = len(df[query].index)\n",
        "        # Random drop starts between 1/12 (o. 25th) and 11/12(o. 275th) packet\n",
        "        randomize_start = lambda: rng.integers(round(size/12), round((11*size)/12), endpoint=True)\n",
        "\n",
        "        # Randomly recovers after 3th to 1/15 (o. 20th) packets\n",
        "        randomize_length = lambda: rng.integers(3, round(size/15), endpoint=True)\n",
        "\n",
        "        start = randomize_start()\n",
        "        stop = start + randomize_length()\n",
        "        query = query & (df.seq >= start) & (df.seq <= stop)\n",
        "        df.loc[query, 'anomaly'] = True\n",
        "\n",
        "        ## Now actually inject anomalies\n",
        "\n",
        "        # get delta between median and noise floor.\n",
        "        delta = abs(RSSI_BASE - df.loc[query, 'rss'].median())\n",
        "\n",
        "        # Drop should be around 30 or less, depending on delta\n",
        "        delta = delta if delta < 30 else (30 - rng.uniform(0, 3))\n",
        "\n",
        "        # Apply drop where anomaly should have appeared\n",
        "        df.loc[query & df.anomaly, 'rss'] -= delta\n",
        "\n",
        "        # Fix values below noise floor\n",
        "        df.loc[query & df.anomaly & (df['rss'] < RSSI_BASE), 'rss'] = RSSI_BASE\n",
        "\n",
        "    assert np.any(df.original != df.rss)\n",
        "\n",
        "    # Scale links afterwards\n",
        "    if scaler:\n",
        "        for src, dst, noise in unique_pairs:\n",
        "            query = (df.src==src) & (df.dst==dst) & (df.noise==noise)\n",
        "            df.loc[query, ['rss']] = scaler.fit_transform(df.loc[query, ['rss']])\n",
        "\n",
        "    assert len(df.index) != 0\n",
        "    return df, lengths_original\n"
      ],
      "metadata": {
        "id": "VeGPyDsGbCQz"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pandas.core.indexers import length_of_indexer\n",
        "def spike_anomaly_injector(scaler=None, random_state=None) -> pd.DataFrame:\n",
        "\n",
        "    # Get cut-ready-to-use prepared dataset for anomaly injection\n",
        "    df, lengths_original = get_random_dataset(1)\n",
        "\n",
        "    # Random number generator\n",
        "    rng = np.random.default_rng(random_state)\n",
        "\n",
        "    # Now, find unique pairs\n",
        "    unique_pairs = get_unique_pairs(df, 1) #df.apply(lambda row: (row['src'], row['dst'], row['noise']), axis=1).unique()\n",
        "    n_pairs = len(unique_pairs)\n",
        "\n",
        "    df['anomaly'] = False\n",
        "    df['type']= 3\n",
        "    df['original'] = df.rss\n",
        "\n",
        "    # approx. 33.3% of links will have anomaly\n",
        "    n_anomaly_pairs = int(n_pairs // 3)\n",
        "    \n",
        "\n",
        "    # Randomly select links, which will be altered\n",
        "    affected_links = rng.choice(unique_pairs, n_anomaly_pairs)\n",
        "    affected_links = check_line(affected_links)\n",
        "\n",
        "    for src, dst, noise in affected_links:\n",
        "        query = (df.src==src) & (df.dst==dst) & (df.noise==noise)\n",
        "        size = len(df[query].index)\n",
        "        # The probability for spike anomaly to appear is:\n",
        "        anomaly_probability = 3. / size\n",
        "        df.loc[query, 'anomaly'] = np.random.choice([False, True], p=[1. - anomaly_probability, anomaly_probability], size=size)\n",
        "\n",
        "        ## Now inject actual annomaly\n",
        "\n",
        "        # get delta between median and noise floor.\n",
        "        delta = abs(RSSI_BASE - df.loc[query, 'rss'].median())\n",
        "\n",
        "        # Drop should be around 30 or less, depending on delta\n",
        "        delta = delta if delta < 30 else (30 - rng.uniform(0, 3))\n",
        "\n",
        "        # Apply drop where anomaly should have appeared\n",
        "        df.loc[query & df.anomaly, 'rss'] -= delta\n",
        "\n",
        "        # Fix values below noise floor\n",
        "        df.loc[query & df.anomaly & (df['rss'] < RSSI_BASE), 'rss'] = RSSI_BASE\n",
        "\n",
        "    assert np.any(df.original != df.rss)\n",
        "\n",
        "    # Scale links afterwards\n",
        "    if scaler:\n",
        "        for src, dst, noise in unique_pairs:\n",
        "            query = (df.src==src) & (df.dst==dst) & (df.noise==noise)\n",
        "            df.loc[query, ['rss']] = scaler.fit_transform(df.loc[query, ['rss']])\n",
        "\n",
        "    assert len(df.index) != 0\n",
        "    return df, lengths_original"
      ],
      "metadata": {
        "id": "UTuEOAV0bCQ0"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def slow_anomaly_injector(scaler=None, random_state=None) -> pd.DataFrame:\n",
        "    # Get ready-to-use prepared dataset for anomaly injection\n",
        "    df, lengths_original = get_random_dataset(1)\n",
        "\n",
        "    # Random number generator\n",
        "    rng = np.random.default_rng(random_state)\n",
        "\n",
        "    # Now, find unique pairs\n",
        "    unique_pairs = get_unique_pairs(df, 1) #df.apply(lambda row: (row['src'], row['dst'], row['noise']), axis=1).unique()\n",
        "    n_pairs = len(unique_pairs)\n",
        "\n",
        "    df['anomaly'] = False\n",
        "    df['type']= 4\n",
        "    df['original'] = df.rss\n",
        "\n",
        "    # approx. 33.3% of links will have anomaly\n",
        "    n_anomaly_pairs = int(n_pairs // 3)\n",
        "\n",
        "    def slope(seq, rate, start):\n",
        "        # General curve\n",
        "        curve = rate * (seq - start)\n",
        "        curve[curve > 0] = 0  # Correction #1: Remove values above 0\n",
        "        #curve[curve < (rate * (end - start))] = rate * (end - start) # Correction #2: stop falling after \"end\" value\n",
        "\n",
        "        # Sanity checks\n",
        "        assert np.all(rate < 0), f'Rate should have been negative. Got {rate}'\n",
        "\n",
        "        return curve\n",
        "\n",
        "    # Randomly select affected links\n",
        "    affected_links = rng.choice(unique_pairs, n_anomaly_pairs)\n",
        "    affected_links = check_line(affected_links)\n",
        "    \n",
        "    for src, dst, noise in affected_links:\n",
        "        query = (df.src==src) & (df.dst==dst) & (df.noise==noise)\n",
        "        size = len(df[query].index)\n",
        "        # randomize start of slow degradation between LENGTH/30 (o. 10th) and LENGTH/3 (o. 200th) packet\n",
        "        rand_start = lambda: rng.integers(0, round(size/15), endpoint=True)\n",
        "        rand_duration = lambda: rng.integers(round(size/2), round((14*size)/15), endpoint=True)\n",
        "        rand_rate = lambda: rng.uniform(0.5, 1.5, size=size) / -size\n",
        "\n",
        "\n",
        "        start = rand_start()\n",
        "        end = start + rand_duration()\n",
        "        rate = rand_rate()\n",
        "\n",
        "        df.loc[query & (df.seq >= start), 'anomaly'] = True\n",
        "        df.loc[query, 'rss'] += slope(seq=df.loc[query].seq, rate=rate, start=start)\n",
        "        #df.loc[query, 'rss'] += factor(seq=df.loc[query,:].seq, offset=start)\n",
        "\n",
        "    assert np.any(df.original != df.rss)\n",
        "\n",
        "    # Scale links afterwards\n",
        "    if scaler:\n",
        "        for src, dst, noise in unique_pairs:\n",
        "            query = (df.src==src) & (df.dst==dst) & (df.noise==noise)\n",
        "            df.loc[query, ['rss']] = scaler.fit_transform(df.loc[query, ['rss']])\n",
        "\n",
        "    assert len(df.index) != 0\n",
        "    return df, lengths_original\n"
      ],
      "metadata": {
        "id": "_5X_KVBXbCQ2"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_all():\n",
        "\n",
        "  df1, lengths_original1 = norecovery_anomaly_injector()\n",
        "  df2, lengths_original2 = recovery_anomaly_injector()\n",
        "  df3, lengths_original3 = spike_anomaly_injector()\n",
        "  df4, lengths_original4 = slow_anomaly_injector()\n",
        "\n",
        "  lo1 = pd.DataFrame(lengths_original1)[2]\n",
        "  lo2 = pd.DataFrame(lengths_original2)[2]\n",
        "  lo3 = pd.DataFrame(lengths_original3)[2]\n",
        "  lo4 = pd.DataFrame(lengths_original4)[2]\n",
        "\n",
        "  lo = lo1.append(lo2).append(lo3).append(lo4,ignore_index=True)\n",
        "  df = df1.append(df2).append(df3).append(df4,ignore_index=True)\n",
        "\n",
        "  np_rss = df['rss'].to_numpy()\n",
        "  np_anomaly = df['anomaly'].to_numpy()\n",
        "  np_type = df['type'].to_numpy()\n",
        "\n",
        "  #and anomaly \n",
        "  anomaly_np = []\n",
        "  temp_len = 0\n",
        "  t = 0\n",
        "  for n in range(NUM_OF_SAMP*4):\n",
        "    for i in range(lo[n]):\n",
        "      t=0\n",
        "      if np_anomaly[temp_len + i] == True:\n",
        "        anomaly_np.append(True)\n",
        "        t=1\n",
        "        break\n",
        "    if t == 0:\n",
        "      anomaly_np.append(False)\n",
        "\n",
        "    temp_len = lo[n] + temp_len\n",
        "  anomaly_df = pd.DataFrame(anomaly_np)\n",
        "\n",
        "  #and type\n",
        "  type_np = []\n",
        "  temp_len = 0\n",
        "  for n in range(NUM_OF_SAMP*4):\n",
        "    if anomaly_np[n] == True:\n",
        "      type_np.append(np_type[temp_len])\n",
        "    else:\n",
        "      type_np.append(0)\n",
        "      \n",
        "    temp_len = lo[n] + temp_len\n",
        "  type_df = pd.DataFrame(type_np)\n",
        "\n",
        "  #creating a array with properties\n",
        "  properties = pd.DataFrame([])\n",
        "  properties['length'] = lo\n",
        "  properties['anomaly'] = anomaly_df\n",
        "  properties['type'] = type_df\n",
        "\n",
        "  properties = properties.to_numpy()\n",
        "\n",
        "  #get mask\n",
        "  mask_np = []\n",
        "  for n in range(len(np_anomaly)):\n",
        "    if np_anomaly[n] == True:\n",
        "      mask_np.append(1)\n",
        "    else:\n",
        "      mask_np.append(0)\n",
        "  np_mask = np.array(mask_np)\n",
        "  mask_df = pd.DataFrame(mask_np)\n",
        "  #reshape rss\n",
        "\n",
        "  temp = 0\n",
        "  rss = np.empty((NUM_OF_SAMP*4, 1), dtype=object)\n",
        "  rss = rss.reshape((NUM_OF_SAMP*4, ))\n",
        "  for n in range(NUM_OF_SAMP*4):\n",
        "    rss[n] = np_rss[temp:temp+lo[n]]\n",
        "    temp = temp + lo[n]\n",
        "\n",
        "  #reshape mask\n",
        "  temp = 0\n",
        "  mask = np.empty((NUM_OF_SAMP*4, 1), dtype=object)\n",
        "  mask = mask.reshape((NUM_OF_SAMP*4, ))\n",
        "  for n in range(NUM_OF_SAMP*4):\n",
        "    mask[n] = np_mask[temp:temp+lo[n]]\n",
        "    temp = temp + lo[n]\n",
        "\n",
        "\n",
        "  return properties, rss, mask\n"
      ],
      "metadata": {
        "id": "4Nfr7SUwbCQ5"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setting START, STOP, LENGTH for cut/uncut and download the initial_dataset"
      ],
      "metadata": {
        "id": "RIcn2-5NybdV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For uncut"
      ],
      "metadata": {
        "id": "LR1BTzP7zgam"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#for uncut (original length of 300)\n",
        "START = 0\n",
        "STOP = 300\n",
        "LENGTH = 300"
      ],
      "metadata": {
        "id": "gOiBeKW0yqgs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "For cut (so all samples will be of equal cut length)"
      ],
      "metadata": {
        "id": "YCgjcd0uzgBV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#for cut\n",
        "START, STOP, LENGTH = give_start_stop_length()"
      ],
      "metadata": {
        "id": "ffKqsDq7zfs9"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "For the random dataset you dont have to set it up"
      ],
      "metadata": {
        "id": "GVCoWBkN5D6u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Getting your initial dataset so we dont need to get traces "
      ],
      "metadata": {
        "id": "nGmZ8FyM2eqH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import shutil\n",
        "import pathlib\n",
        "import os\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "#set your own path to file given in github (i had it on my drive)\n",
        "shutil.copy(\"/content/drive/MyDrive/initial_dataset.csv\",\"/content\")\n",
        "\n",
        "initial_dataset = pd.read_csv('initial_dataset.csv',delim_whitespace = False, header = 0)\n",
        "del initial_dataset['Unnamed: 0']"
      ],
      "metadata": {
        "id": "IYX-iFhdyy1F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Getting your dataset"
      ],
      "metadata": {
        "id": "24v0A2dM1R2o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## cut/uncut"
      ],
      "metadata": {
        "id": "USOiX53As8hH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#run this to get dataset for cut/uncut (depends on what you chose it the \"Setting START, STOP, LENGTH ...\")\n",
        "dataset = get_all_cut()"
      ],
      "metadata": {
        "id": "gEZlELAgLUEn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#upload csv\n",
        "dataset.to_csv(\"dataset.csv\")"
      ],
      "metadata": {
        "id": "BPSYKt3CADhA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## random (every sample has its own length)"
      ],
      "metadata": {
        "id": "_N8I9Dkn7HFh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#run this to get dataset for random dataset (consists of properties, rss, and mask)\n",
        "properties, rss, mask = get_all()"
      ],
      "metadata": {
        "id": "I_D7aedk7HFi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.savez(\"dataset_properties\",properties)\n",
        "np.savez(\"dataset_rss\",rss)\n",
        "np.savez(\"dataset_maks\",mask)"
      ],
      "metadata": {
        "id": "yPipJQtRumny"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}